{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install dspy-ai weaviate-client python-dotenv jinja2  > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hcp_0/RAG/.venv/lib/python3.10/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy.retrieve.weaviate_rm import WeaviateRM\n",
    "import weaviate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "headers = {\"X-Openai-Api-Key\":os.getenv(\"OPENAI_API_KEY\")}\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\",additional_headers=headers)\n",
    "\n",
    "GEN_LM = dspy.OpenAI(model=\"gpt-4\",api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "EVAL_LM = dspy.OpenAI(model=\"gpt-3.5-turbo\",api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "RETRIVER_MODEL = WeaviateRM(\"Paul_Graham\",weaviate_client=client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=GEN_LM,rm=RETRIVER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Maybe question from user input\n",
    "\n",
    "_questions = [\"Does graham went to MIT??\",\n",
    "             \"What was the first program graham tried??\",\n",
    "             \"What did he say about AI??\",\n",
    "             \"Did he like paintings??\",\n",
    "             \"Why did graham hated Physics?\"]\n",
    "questions = [dspy.Example(question=question).with_inputs(\"question\") for question in _questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"You will be given relevant context. Generate answers grounded with that context\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    context = dspy.InputField(desc=\"may contain relevant context to the question\")\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=2)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self,question):\n",
    "        _context = self.retrieve(question).passages\n",
    "        context = \".\".join(_context)\n",
    "        _answer = self.generate_answer(context=context,question=question)\n",
    "        return dspy.Prediction(answer = _answer.answer)    # need to be returned as Prediction for evaluator\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalSignature(dspy.Signature):\n",
    "    Context = dspy.InputField(desc=\"The context for answering the question\")\n",
    "    Question = dspy.InputField(desc=\"The question to be answered\")\n",
    "    Answer = dspy.InputField(desc=\"Generated answer to the question\")\n",
    "    Evaluation = dspy.OutputField(desc=\"Rating between 1 and 5.\")\n",
    "\n",
    "def eval_answer(gold,pred):\n",
    "    question = gold.question\n",
    "    answer  = pred.answer\n",
    "    print(f\"Requested Question: {question}\")\n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "\n",
    "    # Parameters for evaluation\n",
    "    detail = \"Is the Answer detail?\"\n",
    "    faithful = \"Is the generated answer grounded in the context? Say no if it includes significant facts not in the context.\"\n",
    "    overall =   f\"Please rate how well this answer answers the question, {question} based on on the context.\\n\"\n",
    "\n",
    "    with dspy.context(lm=EVAL_LM):\n",
    "        context = \".\".join(dspy.Retrieve(k=2)(question).passages)\n",
    "        detail = dspy.ChainOfThought(EvalSignature)(Context=\"N/A\",Question=detail,Answer=answer)\n",
    "        faithful = dspy.ChainOfThought(EvalSignature)(Context=context,Question=faithful,Answer=answer)\n",
    "        overall = dspy.ChainOfThought(EvalSignature)(Context=context,Question=overall,Answer=answer)\n",
    "\n",
    "    total = float(detail.Evaluation) + float(faithful.Evaluation)*2 + float(overall.Evaluation)\n",
    "\n",
    "    print(f\"Detail   Score: {detail.Evaluation}\")\n",
    "    print(f\"Faithful Score: {faithful.Evaluation}\")\n",
    "    print(f\"Overall  Score: {overall.Evaluation}\")\n",
    "\n",
    "    return total/5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested Question: Does graham went to MIT??\n",
      "Generated Answer: No, Graham did not go to MIT. He went to Harvard.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail   Score: 5\n",
      "Faithful Score: 5\n",
      "Overall  Score: 5\n",
      "Requested Question: What was the first program graham tried??\n",
      "Generated Answer: The first program Graham tried was on the IBM 1401.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail   Score: 2\n",
      "Faithful Score: 5\n",
      "Overall  Score: 5\n",
      "Requested Question: What did he say about AI??\n",
      "Generated Answer: He said that the AI being practiced at the time was a hoax. He described this AI as a program that translates a statement into a formal representation and adds it to its knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail   Score: 3\n",
      "Faithful Score: 5\n",
      "Overall  Score: 5\n",
      "Requested Question: Did he like paintings??\n",
      "Generated Answer: Yes, he liked paintings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail   Score: 2\n",
      "Faithful Score: 1\n",
      "Overall  Score: 5\n",
      "Requested Question: Why did graham hated Physics?\n",
      "Generated Answer: The context does not provide information on why Graham hated Physics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.0 / 5  (300.0): 100%|██████████| 5/5 [00:05<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail   Score: 2\n",
      "Faithful Score: 2\n",
      "Overall  Score: 5\n",
      "Average Metric: 15.0 / 5  (300.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/hcp_0/RAG/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:187: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n",
      "/home/hcp_0/RAG/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:263: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['4.0' '3.4' '3.6' '1.8' '2.2']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:, metric_name] = df[metric_name].apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_56e0d th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_56e0d td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_56e0d_row0_col0, #T_56e0d_row0_col1, #T_56e0d_row0_col2, #T_56e0d_row1_col0, #T_56e0d_row1_col1, #T_56e0d_row1_col2, #T_56e0d_row2_col0, #T_56e0d_row2_col1, #T_56e0d_row2_col2, #T_56e0d_row3_col0, #T_56e0d_row3_col1, #T_56e0d_row3_col2, #T_56e0d_row4_col0, #T_56e0d_row4_col1, #T_56e0d_row4_col2 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_56e0d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_56e0d_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_56e0d_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "      <th id=\"T_56e0d_level0_col2\" class=\"col_heading level0 col2\" >eval_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_56e0d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_56e0d_row0_col0\" class=\"data row0 col0\" >Does graham went to MIT??</td>\n",
       "      <td id=\"T_56e0d_row0_col1\" class=\"data row0 col1\" >No, Graham did not go to MIT. He went to Harvard.</td>\n",
       "      <td id=\"T_56e0d_row0_col2\" class=\"data row0 col2\" >4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56e0d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_56e0d_row1_col0\" class=\"data row1 col0\" >What was the first program graham tried??</td>\n",
       "      <td id=\"T_56e0d_row1_col1\" class=\"data row1 col1\" >The first program Graham tried was on the IBM 1401.</td>\n",
       "      <td id=\"T_56e0d_row1_col2\" class=\"data row1 col2\" >3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56e0d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_56e0d_row2_col0\" class=\"data row2 col0\" >What did he say about AI??</td>\n",
       "      <td id=\"T_56e0d_row2_col1\" class=\"data row2 col1\" >He said that the AI being practiced at the time was a hoax. He described this AI as a program that translates a statement into...</td>\n",
       "      <td id=\"T_56e0d_row2_col2\" class=\"data row2 col2\" >3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56e0d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_56e0d_row3_col0\" class=\"data row3 col0\" >Did he like paintings??</td>\n",
       "      <td id=\"T_56e0d_row3_col1\" class=\"data row3 col1\" >Yes, he liked paintings.</td>\n",
       "      <td id=\"T_56e0d_row3_col2\" class=\"data row3 col2\" >1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_56e0d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_56e0d_row4_col0\" class=\"data row4 col0\" >Why did graham hated Physics?</td>\n",
       "      <td id=\"T_56e0d_row4_col1\" class=\"data row4 col1\" >The context does not provide information on why Graham hated Physics.</td>\n",
       "      <td id=\"T_56e0d_row4_col2\" class=\"data row4 col2\" >2.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2c05caceb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "300.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=questions,display_progress=True,display_table=5)\n",
    "evaluate(RAG(),metric=eval_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
